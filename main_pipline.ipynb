{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b1e2f6f",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be74468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# HydraëŠ” ì£¼í”¼í„° ë…¸íŠ¸ë¶ì—ì„œ íŠ¹ë³„í•œ ì´ˆê¸°í™” ë°©ì‹ì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "\n",
    "\n",
    "from modellib import build, loader, objective\n",
    "from modellib.utils import plots, details, history, gcam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ec31c9",
   "metadata": {},
   "source": [
    "## setting cofig load and path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e766e1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ì„¤ì • íŒŒì¼ ë¡œë“œ\n",
    "initialize(config_path=\"conf\", version_base=None) \n",
    "cfg = compose(config_name=\"config\")\n",
    "\n",
    "# --- ê¸°ë³¸ ì„¤ì • ì ìš© ---\n",
    "# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •\n",
    "torch.manual_seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "random.seed(cfg.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(cfg.seed)\n",
    "\n",
    "# GPU ì„¤ì •\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- ê²°ê³¼ ì €ì¥ ê²½ë¡œ ì„¤ì • ---\n",
    "# ì‹¤í—˜ ì´ë¦„ìœ¼ë¡œ ìµœìƒìœ„ ê²°ê³¼ í´ë” ìƒì„±\n",
    "output_dir = Path(f\"results/{cfg.experiment_name}\")\n",
    "hpo_dir = output_dir / cfg.results_subdirs.hpo\n",
    "models_dir = output_dir / cfg.results_subdirs.models\n",
    "metrics_dir = output_dir / cfg.results_subdirs.metrics\n",
    "gradcam_dir = output_dir / cfg.results_subdirs.gradcam\n",
    "\n",
    "# í´ë” ìƒì„±\n",
    "hpo_dir.mkdir(parents=True, exist_ok=True)\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "gradcam_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ‰ ì„¤ì • ë¡œë“œ ì™„ë£Œ! ì‹¤í—˜ '{cfg.experiment_name}'ì„(ë¥¼) ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "print(f\"DEVICE: {DEVICE}\")\n",
    "print(f\"ê²°ê³¼ëŠ” '{output_dir}' í´ë”ì— ì €ì¥ë©ë‹ˆë‹¤.\") in: {RUNS_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474b9f94",
   "metadata": {},
   "source": [
    "# optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6033f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "opt_hp_path = hpo_dir / \"opt_hp.yaml\"\n",
    "\n",
    "if cfg.hpo.enabled:\n",
    "    print(\"\\n--- ğŸš€ 1ë‹¨ê³„: í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”(HPO) ì‹œì‘ ---\")\n",
    "    \n",
    "    # Objective í•¨ìˆ˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "    objective_func = objective.Objective(\n",
    "        config=cfg, # config ì „ì²´ë¥¼ ë„˜ê²¨ search_spaceë¥¼ ì‚¬ìš©í•˜ë„ë¡ í•¨\n",
    "        data_dir=cfg.data_dir,\n",
    "        backbone=cfg.backbone,\n",
    "        max_epochs=cfg.hpo.max_epochs_per_trial,\n",
    "        n_splits=cfg.split_config.n_splits_cv,\n",
    "        metric_to_optimize=cfg.hpo.metric_to_optimize,\n",
    "        test_ratio=cfg.split_config.test_ratio\n",
    "    )\n",
    "\n",
    "    # Optuna Study ìƒì„±\n",
    "    study = optuna.create_study(\n",
    "        study_name=cfg.experiment_name,\n",
    "        direction=objective_func.direction,\n",
    "        sampler=optuna.samplers.TPESampler(seed=cfg.seed),\n",
    "        pruner=optuna.pruners.HyperbandPruner()\n",
    "    )\n",
    "\n",
    "    # HPO ì‹¤í–‰\n",
    "    study.optimize(objective_func, n_trials=cfg.hpo.n_trials)\n",
    "\n",
    "    # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥\n",
    "    best_params = study.best_trial.params\n",
    "    with open(opt_hp_path, 'w') as f:\n",
    "        yaml.dump(best_params, f, default_flow_style=False)\n",
    "        \n",
    "    print(f\"\\nâœ… HPO ì™„ë£Œ! ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ '{opt_hp_path}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(\"ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:\", best_params)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    if 'target_steps_median' not in best_trial.user_attrs:\n",
    "        raise RuntimeError(\"best_trial.user_attrsì— 'target_steps_median'ì´ ì—†ìŠµë‹ˆë‹¤. Objective ì½”ë“œ ìˆ˜ì •ì´ ëˆ„ë½ëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    target_steps_median = int(best_trial.user_attrs['target_steps_median'])\n",
    "    print(f\"ğŸ“Œ median target steps from CV(best trial): {target_steps_median}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n--- â© 1ë‹¨ê³„: HPO ë¹„í™œì„±í™”. ê¸°ì¡´ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤. ---\")\n",
    "    if not opt_hp_path.exists():\n",
    "        raise FileNotFoundError(f\"HPOê°€ ë¹„í™œì„±í™”ë˜ì—ˆì§€ë§Œ '{opt_hp_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"'{opt_hp_path}' íŒŒì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91852c08",
   "metadata": {},
   "source": [
    "# final model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3afd204",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- ğŸš€ 2ë‹¨ê³„: ìµœì¢… ëª¨ë¸ í•™ìŠµ ì‹œì‘ ---\")\n",
    "# 1. ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œë“œ\n",
    "with open(opt_hp_path, 'r') as f:\n",
    "    best_hp = yaml.safe_load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e95b93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ìµœì¢… í•™ìŠµìš© ë°ì´í„° ë¡œë” ì¤€ë¹„\n",
    "# CV ë•Œ ì‚¬ìš©í•œ Train/Val ë°ì´í„°ë¥¼ ëª¨ë‘ í•©ì³ì„œ ìµœì¢… í•™ìŠµì— ì‚¬ìš©\n",
    "# val_ratio=0ìœ¼ë¡œ ì„¤ì •í•˜ë©´, test_ratioë§Œí¼ ë¶„ë¦¬ëœ ë‚˜ë¨¸ì§€ë¥¼ ëª¨ë‘ í•™ìŠµìš©ìœ¼ë¡œ ì‚¬ìš©\n",
    "train_final_loader, _, test_loader, class_names = loader.create_dataloaders(\n",
    "    data_dir=cfg.data_dir,\n",
    "    backbone=cfg.backbone,\n",
    "    batch_size=best_hp['batch_size'],\n",
    "    val_ratio=0, # Val set ì—†ì´ ì „ì²´ë¥¼ í•™ìŠµì— ì‚¬ìš©\n",
    "    test_ratio=cfg.split_config.test_ratio,\n",
    "    test_dir=cfg.test_dir\n",
    ")\n",
    "print(f\"ìµœì¢… í•™ìŠµ ë°ì´í„°ì…‹: {len(train_final_loader.dataset)}ê°œ, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹: {len(test_loader.dataset)}ê°œ\")\n",
    "\n",
    "# steps_per_epoch_full = ë°°ì¹˜ ì—…ë°ì´íŠ¸ ìˆ˜\n",
    "steps_per_epoch_full = len(train_final_loader)\n",
    "if steps_per_epoch_full <= 0:\n",
    "    raise RuntimeError(\"steps_per_epoch_fullì´ 0 ì´í•˜ì…ë‹ˆë‹¤. full-train dataloader êµ¬ì„±ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "epochs_final = int(math.ceil(target_steps_median / steps_per_epoch_full))\n",
    "print(f\"ğŸ¯ epochs_final = ceil({target_steps_median} / {steps_per_epoch_full}) = {epochs_final}\")\n",
    "\n",
    "epochs_final_path = hpo_dir / \"epochs_final.yaml\"\n",
    "with open(epochs_final_path, \"w\") as f:\n",
    "    yaml.dump({\"epochs_final\": epochs_final,\n",
    "               \"target_steps_median\": int(target_steps_median),\n",
    "               \"steps_per_epoch_full\": int(steps_per_epoch_full),\n",
    "               \"fold_best_steps\": best_trial.user_attrs.get(\"fold_best_steps\", []),\n",
    "               \"fold_best_epochs\": best_trial.user_attrs.get(\"fold_best_epochs\", [])}, f)\n",
    "print(f\"ğŸ“ ìµœì¢… í•™ìŠµìš© epoch ì •ë³´ë¥¼ '{epochs_final_path}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa42be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) epochs_final ë¡œë“œ\n",
    "with open(hpo_dir / \"epochs_final.yaml\") as f:\n",
    "    ef = yaml.safe_load(f)\n",
    "epochs_final = int(ef[\"epochs_final\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656ca99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ëª¨ë¸, ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¤„ëŸ¬ ì¤€ë¹„\n",
    "model = build(\n",
    "    backbone=cfg.backbone,\n",
    "    num_classes=len(class_names),\n",
    "    dropout_rate=best_hp['dropout_rate']\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = getattr(optim, best_hp['optimizer'])(\n",
    "    model.parameters(), lr=best_hp['lr'], weight_decay=best_hp.get('weight_decay', 0.0)\n",
    ")\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs_final):\n",
    "    # ìµœì¢… í•™ìŠµì—ì„œëŠ” Train lossë§Œ ê³„ì‚° (Val setì´ ì—†ìœ¼ë¯€ë¡œ)\n",
    "    # ì‹¤ì œë¡œëŠ” validate_one_epochì„ test_loaderë¡œ ëŒë ¤ì„œ ì„±ëŠ¥ì„ ëª¨ë‹ˆí„°ë§ í•  ìˆ˜ ìˆìŒ\n",
    "    train_loss = objective.train_one_epoch(model, train_final_loader, criterion, optimizer, DEVICE)\n",
    "    val_loss, macro_f1 = objective.validate_one_epoch(model, test_loader, criterion, DEVICE) # í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ê²€ì¦\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs_final} | Train Loss: {train_loss:.4f} | Test Loss: {val_loss:.4f} | Test Macro-F1: {macro_f1:.4f}\")\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['macro_f1'].append(macro_f1)\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "print(f\"\\nâœ… ìµœì¢… í•™ìŠµ ì™„ë£Œ! ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ì„ '{models_dir}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8912ffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- ğŸ“‹ Saving Model Details ---\")\n",
    "\n",
    "# ëª¨ë¸ ìƒì„¸ ì •ë³´(íŒŒë¼ë¯¸í„° ìˆ˜, ì—°ì‚°ëŸ‰ ë“±) ì €ì¥\n",
    "details.save_model_details(\n",
    "    model=model,\n",
    "    save_dir=metrics_dir,\n",
    "    model_name=cfg.experiment_name,\n",
    "    input_size=(1, 3, 224, 224) # ëª¨ë¸ì— ë§ëŠ” ì‚¬ì´ì¦ˆë¡œ ì¡°ì ˆ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a9851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  íˆìŠ¤í† ë¦¬ ê·¸ë˜í”„ ì €ì¥ ë° í™•ì¸\n",
    "history_plot_path = metrics_dir / f\"{cfg.experiment_name}_history.png\"\n",
    "history.save_history_plot(\n",
    "    history=history,\n",
    "    save_dir=metrics_dir,\n",
    "    model_name=cfg.experiment_name\n",
    ")\n",
    "display.Image(filename=history_plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7e411",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473449bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- ğŸ“Š Evaluating Final Model on Test Set ---\")\n",
    "\n",
    "# í‰ê°€ ê²°ê³¼ë¬¼(ë¦¬í¬íŠ¸, í–‰ë ¬, ê³¡ì„ ) ì €ì¥\n",
    "plots.save_classification_results(\n",
    "    model=model,\n",
    "    dataloader=test_loader,\n",
    "    class_names=class_names,\n",
    "    save_dir=metrics_dir,\n",
    "    model_name=cfg.experiment_name,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d4d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- ğŸ‘ï¸â€ğŸ—¨ï¸ Generating Grad-CAM Visualizations ---\")\n",
    "\n",
    "gcam.visualize_and_save_gcam_results(\n",
    "    model=model,\n",
    "    dataloader=test_loader,\n",
    "    class_names=class_names,\n",
    "    save_dir=gradcam_dir,\n",
    "    device=DEVICE,\n",
    "    n_true_preds=5,\n",
    "    n_misclassified=5\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ‰ Grad-CAM ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d510da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nğŸ‰ğŸ‰ğŸ‰ All processes are complete! ğŸ‰ğŸ‰ğŸ‰\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
